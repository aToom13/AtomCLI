/**
 * PromptManager â€” Unified Prompt Orchestration System
 *
 * Single entry point for all system prompt composition. Merges the former
 * builder.ts (which imported .txt files) with the modular manager architecture.
 *
 * Architecture:
 *
 *   prompt/
 *   â”œâ”€â”€ core/           8 base .txt prompts (always included)
 *   â”œâ”€â”€ provider/       4 provider-specific .txt prompts
 *   â”œâ”€â”€ agent/          4 agent-mode .txt prompts
 *   â”œâ”€â”€ runtime/        Special-purpose runtime injections
 *   â””â”€â”€ manager.ts      â† THIS FILE (unified orchestrator)
 *
 * Modules:
 *   IdentityModule      â†’ core/identity.txt
 *   ToolsModule         â†’ core/tools.txt + orchestrate/todo extras
 *   WorkflowModule      â†’ core/workflow.txt
 *   CommunicationModule â†’ core/communication.txt
 *   CodeEditingModule   â†’ core/code-editing.txt + read-before-edit emphasis
 *   GitSafetyModule     â†’ core/git-safety.txt
 *   ExtensionsModule    â†’ core/extensions.txt (skills + MCP)
 *   SelfLearningModule  â†’ core/self-learning.txt
 *   ContextModule       â†’ Dynamic env (CWD, OS, date, git, user profile)
 *   ProviderModule      â†’ provider/*.txt (auto-detected from model ID)
 *   AgentModule         â†’ agent/*.txt (based on agent type)
 */

import fs from "fs/promises"
import path from "path"
import os from "os"

// â”€â”€â”€ Core Prompts (always included) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import PROMPT_IDENTITY from "./core/identity.txt"
import PROMPT_TOOLS from "./core/tools.txt"
import PROMPT_WORKFLOW from "./core/workflow.txt"
import PROMPT_COMMUNICATION from "./core/communication.txt"
import PROMPT_CODE_EDITING from "./core/code-editing.txt"
import PROMPT_GIT_SAFETY from "./core/git-safety.txt"
import PROMPT_EXTENSIONS from "./core/extensions.txt"
import PROMPT_SELF_LEARNING from "./core/self-learning.txt"

// â”€â”€â”€ Provider-Specific Prompts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import PROMPT_ANTHROPIC from "./provider/anthropic.txt"
import PROMPT_GEMINI from "./provider/gemini.txt"
import PROMPT_OPENAI from "./provider/openai.txt"
import PROMPT_GENERIC from "./provider/generic.txt"

// â”€â”€â”€ Agent-Specific Prompts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import PROMPT_AGENT from "./agent/agent.txt"
import PROMPT_EXPLORE from "./agent/explore.txt"
import PROMPT_PLAN from "./agent/plan.txt"
import PROMPT_BUILD from "./agent/build.txt"

// â”€â”€â”€ Learning System â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import { Learning } from "@/services/learning"

// â”€â”€â”€ Types â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export type ProviderType = "anthropic" | "gemini" | "openai" | "generic"
export type AgentType = "agent" | "explore" | "plan" | "build"

export interface BuildOptions {
    /** Model API ID (e.g., "claude-3-5-sonnet", "gemini-2.0-flash") */
    modelId: string
    /** Agent type (default: "agent") */
    agent?: AgentType
    /** Custom sections to append */
    customSections?: string[]
    /** Include learning memory summary */
    includeLearningMemory?: boolean
    /** Include user profile */
    includeUserProfile?: boolean
}

// â”€â”€â”€ Provider Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function detectProvider(modelId: string): ProviderType {
    const id = modelId.toLowerCase()
    if (id.includes("claude")) return "anthropic"
    if (id.includes("gemini")) return "gemini"
    if (id.includes("gpt") || id.includes("o1") || id.includes("o3") || id.includes("o4")) return "openai"
    return "generic"
}

// â”€â”€â”€ Prompt Maps â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const PROVIDER_PROMPTS: Record<ProviderType, string> = {
    anthropic: PROMPT_ANTHROPIC,
    gemini: PROMPT_GEMINI,
    openai: PROMPT_OPENAI,
    generic: PROMPT_GENERIC,
}

const AGENT_PROMPTS: Record<AgentType, string> = {
    agent: PROMPT_AGENT,
    explore: PROMPT_EXPLORE,
    plan: PROMPT_PLAN,
    build: PROMPT_BUILD,
}

/**
 * Core prompts that are ALWAYS included in every system prompt.
 * Order matters â€” identity first, then capabilities, then rules.
 */
const CORE_PROMPTS = [
    PROMPT_IDENTITY,
    PROMPT_SELF_LEARNING,
    PROMPT_TOOLS,
    PROMPT_WORKFLOW,
    PROMPT_COMMUNICATION,
    PROMPT_CODE_EDITING,
    PROMPT_GIT_SAFETY,
    PROMPT_EXTENSIONS,
]

// â”€â”€â”€ Read-Before-Edit Emphasis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const READ_BEFORE_EDIT_EMPHASIS = `
<critical_rule>
## â›” THE GOLDEN RULE â€” READ BEFORE EDIT

This is the single most critical rule in your entire system prompt.

\`\`\`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                              â”‚
â”‚   You MUST Read() a file BEFORE you can Edit() it.          â”‚
â”‚                                                              â”‚
â”‚   The Edit tool WILL FAIL if you haven't read first.        â”‚
â”‚   This is a hard technical constraint, not a suggestion.    â”‚
â”‚                                                              â”‚
â”‚   BEFORE every edit, verify:                                â”‚
â”‚   1. Have I Read() this file in this session?               â”‚
â”‚   2. Has the file changed since I last read it?             â”‚
â”‚   3. Do I understand the code patterns in this file?        â”‚
â”‚   4. Does my oldString EXACTLY match current content?       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
\`\`\`
</critical_rule>
`.trim()

// â”€â”€â”€ Orchestrate Tool Details â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const ORCHESTRATE_DETAILS = `
<orchestrate_guide>
## ğŸ¯ Orchestrate Tool â€” Multi-Agent Workflow Engine

The \`orchestrate\` tool decomposes complex tasks into parallel and sequential sub-tasks,
each executed by a dedicated sub-agent.

### When to Use
Use when a request can be broken into **multiple independent subtasks**:
- "Analyze code, write tests, and create docs" â†’ 3 tasks, tests+docs depend on analysis
- "Refactor module A and B, then integrate" â†’ 2 parallel + 1 dependent task

### How to Use (2 Steps)

**Step 1 â€” Plan:**
\`\`\`json
{
  "action": "plan",
  "tasks": [
    { "id": "analyze", "prompt": "Analyze the codebase", "category": "analysis" },
    { "id": "tests", "prompt": "Write tests", "category": "coding", "dependsOn": ["analyze"] },
    { "id": "docs", "prompt": "Write docs", "category": "documentation", "dependsOn": ["analyze"] }
  ]
}
\`\`\`
â†’ Returns \`workflowId\`. Same-layer tasks run in **parallel**.

**Step 2 â€” Execute:**
\`\`\`json
{ "action": "execute", "workflowId": "<returned-id>" }
\`\`\`
â†’ Runs in background. You get notified with results + Session IDs.

**Optional â€” Status / Abort:**
\`\`\`json
{ "action": "status", "workflowId": "<id>" }
{ "action": "abort", "workflowId": "<id>" }
{ "action": "abort", "sessionId": "<sub-agent-session-id>" }
\`\`\`

### Task Categories
\`"coding"\` | \`"documentation"\` | \`"analysis"\` | \`"general"\`

### Task vs Orchestrate
- **Task**: Single focused subtask, blocking (waits for result)
- **Orchestrate**: Multiple subtasks, non-blocking (background execution)
</orchestrate_guide>
`.trim()

// â”€â”€â”€ TodoWrite Details â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const TODOWRITE_DETAILS = `
<chain_todo_guide>
## ğŸ“‹ Chain + TodoWrite â€” Visual Progress + Task Tracking

You have TWO complementary tools for task management:

### ğŸ”— Chain (chainupdate) â€” What the USER sees
Visual progress bar at the top of the terminal. **Always use for user visibility.**
\`\`\`
chainupdate [action=start]                        â†’ Start progress bar
chainupdate [action=update, status=doing_X]       â†’ Show current action
chainupdate [action=complete]                     â†’ Advance progress
chainupdate [action=clear]                        â†’ Clear when done
\`\`\`

### ğŸ“‹ TodoWrite â€” Task state tracking
Structured task list with states. **Use for planning and state management.**
\`\`\`typescript
TodoWrite([
  { id: "1", content: "ğŸ” Analyze", status: "in_progress" },  // âš ï¸ ONLY ONE!
  { id: "2", content: "ğŸ“ Implement", status: "pending" },
  { id: "3", content: "âœ… Verify", status: "pending" }
])
\`\`\`

### ğŸ”„ Use BOTH Together
1. **Start**: \`chainupdate [action=start]\` + TodoWrite with plan
2. **Progress**: \`chainupdate [action=update]\` for visual + TodoWrite for state
3. **Step done**: \`chainupdate [action=complete]\` + TodoWrite mark completed
4. **Finish**: \`chainupdate [action=clear]\` + TodoWrite all completed

### Rules
- Only ONE \`in_progress\` task at a time
- Mark \`in_progress\` BEFORE starting, \`completed\` IMMEDIATELY after
- Chain = user visibility, TodoWrite = state tracking
</chain_todo_guide>
`.trim()

// â”€â”€â”€ Dynamic Context Generator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function generateDynamicContext(): Promise<string> {
    const parts: string[] = []

    // User profile
    try {
        const profilePath = path.join(os.homedir(), ".atomcli", "personality", "user-profile.json")
        const data = await fs.readFile(profilePath, "utf-8")
        const profile = JSON.parse(data)

        const profileParts = ["<user_context>", "# USER CONTEXT"]
        if (profile.name) profileParts.push(`- **Name**: ${profile.name}`)
        if (profile.techLevel) profileParts.push(`- **Tech Level**: ${profile.techLevel}`)
        if (profile.communication) profileParts.push(`- **Communication Style**: ${profile.communication}`)
        if (profile.primaryLanguage) profileParts.push(`- **Primary Language**: ${profile.primaryLanguage}`)
        if (profile.interests?.length > 0) profileParts.push(`- **Interests**: ${profile.interests.join(", ")}`)
        profileParts.push("</user_context>")

        parts.push(profileParts.join("\n"))
    } catch {
        // No user profile â€” that's fine
    }

    // Learning memory
    try {
        const memorySummary = await Learning.buildMemorySummary()
        if (memorySummary) {
            parts.push(`<learning_memory>\n${memorySummary}\n</learning_memory>`)
        }
    } catch {
        // No learning data â€” that's fine
    }

    return parts.join("\n\n")
}

// â”€â”€â”€ PromptManager (Main Export) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * Builds a complete system prompt synchronously.
 * Uses .txt file imports + inline emphasis sections.
 */
function build(options: BuildOptions): string {
    const { modelId, agent = "agent", customSections = [] } = options
    const provider = detectProvider(modelId)

    const sections: string[] = [
        // Core prompts from .txt files (identity, tools, workflow, etc.)
        ...CORE_PROMPTS,
        // Critical emphasis: read-before-edit
        READ_BEFORE_EDIT_EMPHASIS,
        // Detailed orchestrate instructions
        ORCHESTRATE_DETAILS,
        // Detailed TodoWrite instructions
        TODOWRITE_DETAILS,
        // Provider-specific optimizations
        PROVIDER_PROMPTS[provider],
        // Agent-specific behavior
        AGENT_PROMPTS[agent],
        // Custom sections (user-provided extras)
        ...customSections,
    ]

    return sections.filter(Boolean).join("\n\n---\n\n")
}

/**
 * Builds a complete system prompt asynchronously.
 * Includes dynamic context (user profile, learning memory).
 */
async function buildAsync(options: BuildOptions): Promise<string> {
    const {
        modelId,
        agent = "agent",
        customSections = [],
        includeLearningMemory = true,
        includeUserProfile = true,
    } = options

    const provider = detectProvider(modelId)

    // Load dynamic context
    let dynamicCtx = ""
    if (includeLearningMemory || includeUserProfile) {
        dynamicCtx = await generateDynamicContext()
    }

    const sections: string[] = [
        // Core prompts from .txt files
        ...CORE_PROMPTS,
        // Dynamic context (user profile + learning memory)
        ...(dynamicCtx ? [dynamicCtx] : []),
        // Critical emphasis: read-before-edit
        READ_BEFORE_EDIT_EMPHASIS,
        // Detailed orchestrate instructions
        ORCHESTRATE_DETAILS,
        // Detailed TodoWrite instructions
        TODOWRITE_DETAILS,
        // Provider-specific optimizations
        PROVIDER_PROMPTS[provider],
        // Agent-specific behavior
        AGENT_PROMPTS[agent],
        // Custom sections
        ...customSections,
    ]

    return sections.filter(Boolean).join("\n\n---\n\n")
}

/**
 * Gets the provider prompt for a given model ID.
 */
function getProviderPrompt(modelId: string): string {
    return PROVIDER_PROMPTS[detectProvider(modelId)]
}

/**
 * Gets all core prompts concatenated.
 */
function getBasePrompts(): string {
    return CORE_PROMPTS.join("\n\n---\n\n")
}

/**
 * Gets the agent prompt for a given agent type.
 */
function getAgentPrompt(agent: AgentType): string {
    return AGENT_PROMPTS[agent]
}

/**
 * Returns prompt statistics for debugging/optimization.
 */
function getStats(options: BuildOptions): {
    totalTokens: number
    sections: { name: string; tokens: number; chars: number }[]
} {
    const prompt = build(options)
    const estimateTokens = (text: string) => Math.ceil(text.length / 4)

    const sections = [
        { name: "identity", tokens: estimateTokens(PROMPT_IDENTITY), chars: PROMPT_IDENTITY.length },
        { name: "self-learning", tokens: estimateTokens(PROMPT_SELF_LEARNING), chars: PROMPT_SELF_LEARNING.length },
        { name: "tools", tokens: estimateTokens(PROMPT_TOOLS), chars: PROMPT_TOOLS.length },
        { name: "workflow", tokens: estimateTokens(PROMPT_WORKFLOW), chars: PROMPT_WORKFLOW.length },
        { name: "communication", tokens: estimateTokens(PROMPT_COMMUNICATION), chars: PROMPT_COMMUNICATION.length },
        { name: "code-editing", tokens: estimateTokens(PROMPT_CODE_EDITING), chars: PROMPT_CODE_EDITING.length },
        { name: "git-safety", tokens: estimateTokens(PROMPT_GIT_SAFETY), chars: PROMPT_GIT_SAFETY.length },
        { name: "extensions", tokens: estimateTokens(PROMPT_EXTENSIONS), chars: PROMPT_EXTENSIONS.length },
        { name: "read-before-edit", tokens: estimateTokens(READ_BEFORE_EDIT_EMPHASIS), chars: READ_BEFORE_EDIT_EMPHASIS.length },
        { name: "orchestrate-guide", tokens: estimateTokens(ORCHESTRATE_DETAILS), chars: ORCHESTRATE_DETAILS.length },
        { name: "todowrite-guide", tokens: estimateTokens(TODOWRITE_DETAILS), chars: TODOWRITE_DETAILS.length },
        { name: `provider:${detectProvider(options.modelId)}`, tokens: estimateTokens(PROVIDER_PROMPTS[detectProvider(options.modelId)]), chars: PROVIDER_PROMPTS[detectProvider(options.modelId)].length },
        { name: `agent:${options.agent || "agent"}`, tokens: estimateTokens(AGENT_PROMPTS[options.agent || "agent"]), chars: AGENT_PROMPTS[options.agent || "agent"].length },
    ]

    return {
        totalTokens: estimateTokens(prompt),
        sections,
    }
}

// â”€â”€â”€ Exports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

export const PromptManager = {
    build,
    buildAsync,
    getProviderPrompt,
    getBasePrompts,
    getAgentPrompt,
    getStats,
    detectProvider,
}

/**
 * @deprecated Use PromptManager instead. This alias exists for backward compatibility.
 */
export const PromptBuilder = PromptManager

export default PromptManager
